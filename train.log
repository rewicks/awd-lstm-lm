Loading cached dataset...
Using []
Args: Namespace(alpha=2, batch_size=128, beta=1, bptt=70, clip=0.25, cuda=True, data='/home/hltcoe/rwicks/code/awd-lstm-lm/data/open-subs_news-commentary/', dropout=0.4, dropoute=0.1, dropouth=0.25, dropouti=0.4, emsize=400, epochs=500, log_interval=1000, log_path='/home/hltcoe/rwicks/code/awd-lstm-lm/train.log', lr=0.01, model='LSTM', nhid=1150, nlayers=3, nonmono=5, optimizer='adam', resume='', save='/exp/rwicks/models/language_models/awd-lstm/news-comm_open-subs', seed=141, tied=True, vocab_path='/exp/rwicks/models/spm/unigram/news-commentary_open-subtitles.8000.json', wdecay=1.2e-06, wdrop=0.5, when=[-1])
Model total parameters: 27141682
| epoch   1 |  1000/138538 batches | lr 0.01000 | ms/batch 168.83 | loss  4.86 | ppl   128.84 | bpc    7.009
| epoch   1 |  2000/138538 batches | lr 0.01000 | ms/batch 173.20 | loss  4.34 | ppl    76.94 | bpc    6.266
| epoch   1 |  3000/138538 batches | lr 0.01000 | ms/batch 171.63 | loss  4.21 | ppl    67.29 | bpc    6.072
| epoch   1 |  4000/138538 batches | lr 0.01000 | ms/batch 172.01 | loss  4.18 | ppl    65.56 | bpc    6.035
| epoch   1 |  5000/138538 batches | lr 0.01000 | ms/batch 172.78 | loss  4.16 | ppl    63.94 | bpc    5.999
-----------------------------------------------------------------------------------------
Exiting from training early
